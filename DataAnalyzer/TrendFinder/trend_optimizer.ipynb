{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa3aa23-e2e2-4a6b-94e9-dfb4271bf2b0",
   "metadata": {},
   "source": [
    "Notebook for optimizing the trend analyzer code.\n",
    "To run the notebook, run the command \"jupyter notebook --ip=0.0.0.0 --port=5024 --allow-root --no-browser\". Then open the webpage that opens in replit in a new tab, and enter the token for the server you find from the command \"jupter server list\". If you enter the token in the replit preview it will give you a 403 error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868f8830-df75-40f4-844e-371030acf6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "from modules import *\n",
    "import os\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd921eb-5851-448a-9dd4-aa8ece445a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup mlflow\n",
    "mlflow_tracking_uri = \"../../mlflow/experiments\"\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_experiment(\"TrendFinderOptmizer\")\n",
    "mlflow.dspy.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fb65a5-c9f6-453d-9284-fad6223d3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup dspy\n",
    "api_key = os.environ['paul2']\n",
    "lm = dspy.LM('gemini/gemini-2.5-flash', api_key=api_key, max_tokens=8000)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15e81f-2189-4043-a8b7-2aebee69e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_trends = trend_analyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimization-intro",
   "metadata": {},
   "source": [
    "## DSPy Optimization for doc_analyzer\n",
    "\n",
    "Since we don't have labeled training data, we'll use DSPy's signature optimization and prompt engineering techniques to improve the `doc_analyzer` signature performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import optimization modules\n",
    "import dspy\n",
    "from dspy.teleprompt import BootstrapFewShot, SignatureOptimizer\n",
    "from dspy.evaluate import Evaluate\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-synthetic-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic examples for different document types and categories\n",
    "# This helps DSPy understand the expected input/output patterns\n",
    "\n",
    "def create_synthetic_examples():\n",
    "    examples = []\n",
    "    \n",
    "    # Example 1: Financial document analysis\n",
    "    examples.append(dspy.Example(\n",
    "        document=\"Sample financial report with revenue: $50,000, expenses: $30,000, profit: $20,000\",\n",
    "        categories=[\"revenue\", \"expenses\", \"profit\"],\n",
    "        in_csv=\"\",\n",
    "        last_context=\"Analyzing quarterly financial reports\",\n",
    "        next_context=\"Found consistent profit margins across documents\",\n",
    "        out_csv=\"document,revenue,expenses,profit\\nfinancial_report.pdf,$50000,$30000,$20000\"\n",
    "    ))\n",
    "    \n",
    "    # Example 2: Employee data analysis\n",
    "    examples.append(dspy.Example(\n",
    "        document=\"Employee record: John Doe, Department: Engineering, Salary: $75,000, Years: 3\",\n",
    "        categories=[\"name\", \"department\", \"salary\", \"experience\"],\n",
    "        in_csv=\"\",\n",
    "        last_context=\"Processing employee database for HR analysis\",\n",
    "        next_context=\"Engineering department shows higher average salaries\",\n",
    "        out_csv=\"document,name,department,salary,experience\\nemployee_data.csv,John Doe,Engineering,$75000,3 years\"\n",
    "    ))\n",
    "    \n",
    "    # Example 3: Inspection report (similar to your use case)\n",
    "    examples.append(dspy.Example(\n",
    "        document=\"Inspection Report ID: 32037, Violations: 2, Details: Missing safety equipment, Improper storage\",\n",
    "        categories=[\"document\", \"number of violations\", \"list and details of violations\"],\n",
    "        in_csv=\"\",\n",
    "        last_context=\"Analyzing safety inspection reports for compliance trends\",\n",
    "        next_context=\"Safety equipment violations are common across multiple sites\",\n",
    "        out_csv=\"document,number of violations,list and details of violations\\n32037_inspection.pdf,2,Missing safety equipment; Improper storage\"\n",
    "    ))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "synthetic_examples = create_synthetic_examples()\n",
    "print(f\"Created {len(synthetic_examples)} synthetic examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics for the doc_analyzer\n",
    "def evaluate_doc_analyzer_output(example, pred, trace=None):\n",
    "    \"\"\"Evaluate the quality of doc_analyzer output\"\"\"\n",
    "    score = 0.0\n",
    "    max_score = 4.0\n",
    "    \n",
    "    # Check if CSV format is valid\n",
    "    try:\n",
    "        import io\n",
    "        pd.read_csv(io.StringIO(pred.out_csv))\n",
    "        score += 1.0  # Valid CSV format\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Check if context is updated (not identical to input)\n",
    "    if pred.next_context != example.last_context and len(pred.next_context.strip()) > 0:\n",
    "        score += 1.0\n",
    "    \n",
    "    # Check if all categories are addressed in CSV\n",
    "    try:\n",
    "        csv_data = pd.read_csv(io.StringIO(pred.out_csv))\n",
    "        csv_columns = set(csv_data.columns.tolist())\n",
    "        expected_categories = set(example.categories)\n",
    "        if expected_categories.issubset(csv_columns):\n",
    "            score += 1.0\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Check if output contains meaningful data (not just N/A)\n",
    "    if \"N/A\" not in pred.out_csv or pred.out_csv.count(\"N/A\") < len(example.categories):\n",
    "        score += 1.0\n",
    "    \n",
    "    return score / max_score\n",
    "\n",
    "print(\"Evaluation metric defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimize-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the doc_analyzer signature\n",
    "# We'll use SignatureOptimizer to improve the prompt and reasoning\n",
    "\n",
    "# Create an instance of the doc_analyzer module\n",
    "doc_analyzer_module = dspy.ChainOfThought(doc_analyzer)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = SignatureOptimizer(\n",
    "    metric=evaluate_doc_analyzer_output,\n",
    "    breadth=10,  # Number of candidate prompts to generate\n",
    "    depth=2,     # Number of optimization rounds\n",
    "    init_temperature=1.4,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Starting signature optimization...\")\n",
    "print(\"This may take several minutes as it tests different prompt variations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the optimization with MLflow tracking\n",
    "with mlflow.start_run(run_name=\"doc_analyzer_optimization\") as run:\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"optimizer_type\", \"SignatureOptimizer\")\n",
    "    mlflow.log_param(\"num_examples\", len(synthetic_examples))\n",
    "    mlflow.log_param(\"breadth\", 10)\n",
    "    mlflow.log_param(\"depth\", 2)\n",
    "    \n",
    "    # Optimize the signature\n",
    "    optimized_module = optimizer.compile(\n",
    "        doc_analyzer_module,\n",
    "        trainset=synthetic_examples[:2],  # Use first 2 for training\n",
    "        valset=synthetic_examples[2:],    # Use remaining for validation\n",
    "    )\n",
    "    \n",
    "    # Log the optimized signature\n",
    "    mlflow.log_text(str(optimized_module.signature), \"optimized_signature.txt\")\n",
    "    \n",
    "    print(\"Optimization completed!\")\n",
    "    print(f\"Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimized module with a sample\n",
    "test_example = synthetic_examples[0]\n",
    "\n",
    "print(\"Testing Original Module:\")\n",
    "print(\"=\" * 40)\n",
    "original_result = doc_analyzer_module(\n",
    "    document=test_example.document,\n",
    "    categories=test_example.categories,\n",
    "    in_csv=test_example.in_csv,\n",
    "    last_context=test_example.last_context\n",
    ")\n",
    "print(f\"Context: {original_result.next_context}\")\n",
    "print(f\"CSV: {original_result.out_csv}\")\n",
    "print(f\"Score: {evaluate_doc_analyzer_output(test_example, original_result)}\")\n",
    "\n",
    "print(\"\\nTesting Optimized Module:\")\n",
    "print(\"=\" * 40)\n",
    "optimized_result = optimized_module(\n",
    "    document=test_example.document,\n",
    "    categories=test_example.categories,\n",
    "    in_csv=test_example.in_csv,\n",
    "    last_context=test_example.last_context\n",
    ")\n",
    "print(f\"Context: {optimized_result.next_context}\")\n",
    "print(f\"CSV: {optimized_result.out_csv}\")\n",
    "print(f\"Score: {evaluate_doc_analyzer_output(test_example, optimized_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-optimized-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized module for use in the main application\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create optimized modules directory if it doesn't exist\n",
    "os.makedirs(\"optimized_modules\", exist_ok=True)\n",
    "\n",
    "# Save the optimized module\n",
    "with open(\"optimized_modules/optimized_doc_analyzer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(optimized_module, f)\n",
    "\n",
    "# Also save the signature as text for inspection\n",
    "with open(\"optimized_modules/optimized_signature.txt\", \"w\") as f:\n",
    "    f.write(str(optimized_module.signature))\n",
    "\n",
    "print(\"Optimized module saved!\")\n",
    "print(\"You can load it in your main application using:\")\n",
    "print(\"with open('optimized_modules/optimized_doc_analyzer.pkl', 'rb') as f:\")\n",
    "print(\"    optimized_doc_analyzer = pickle.load(f)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-with-real-documents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimized module with your actual documents\n",
    "print(\"Testing with real documents from your training data...\")\n",
    "\n",
    "# Use the same setup as in your test.py\n",
    "categories = [\n",
    "    \"document\", \"number of violations\", \"list and details of violations\"\n",
    "]\n",
    "\n",
    "documents = []\n",
    "documents.append(\n",
    "    Attachments(\n",
    "        \"TrainingData/32037 ALR10C6BZ 097 07-10-2025 INSPR AEPACS NA.pdf\"))\n",
    "\n",
    "# Create an optimized trend analyzer\n",
    "class optimized_trend_analyzer(dspy.Module):\n",
    "    def __init__(self, optimized_doc_analyzer):\n",
    "        super().__init__()\n",
    "        self.doc_analyzer_sql = optimized_doc_analyzer\n",
    "\n",
    "    def forward(self, documents: list[Attachments], categories: list[str], context: str):\n",
    "        doc_summary = \"\"\n",
    "        for document in documents:\n",
    "            result = self.doc_analyzer_sql(\n",
    "                document=document,\n",
    "                categories=categories,\n",
    "                in_csv=doc_summary,\n",
    "                last_context=context\n",
    "            )\n",
    "            context = result.next_context\n",
    "            doc_summary = result.out_csv\n",
    "        return doc_summary, context\n",
    "\n",
    "# Test with optimized module\n",
    "optimized_analyzer = optimized_trend_analyzer(optimized_module)\n",
    "result, context = optimized_analyzer(\n",
    "    documents=documents[:1],  # Test with first document\n",
    "    categories=categories,\n",
    "    context=\"\"\n",
    ")\n",
    "\n",
    "print(\"Optimized Result:\")\n",
    "print(f\"CSV Output: {result}\")\n",
    "print(f\"Context: {context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimization-summary",
   "metadata": {},
   "source": [
    "## Optimization Summary\n",
    "\n",
    "This notebook optimizes the `doc_analyzer` signature using:\n",
    "\n",
    "1. **Synthetic Examples**: Created representative examples for different document types\n",
    "2. **SignatureOptimizer**: Improved the prompt and reasoning chains\n",
    "3. **Custom Metrics**: Evaluated CSV format, context updates, and data extraction quality\n",
    "4. **MLflow Tracking**: Logged optimization experiments for comparison\n",
    "\n",
    "The optimized module should perform better at:\n",
    "- Extracting relevant information from documents\n",
    "- Generating proper CSV format\n",
    "- Maintaining meaningful context across documents\n",
    "- Handling various document types and categories\n",
    "\n",
    "To use the optimized module in production, load the saved pickle file and replace the original `doc_analyzer` in your `trend_analyzer` class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
